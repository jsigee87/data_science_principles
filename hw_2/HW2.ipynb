{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 461P: Data Science Principles</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 85</p>\n",
    "## <p style=\"text-align: center;\">Due: Tuesday, October 2nd, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  **Please include the name and UTEID for both students on all submitted files (including this notebook).**\n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 0. Bias-variance (15pts)\n",
    "Use the following code to read in a small set of data and divide it into training and testing sets. Inputs are x; outputs are y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_train = np.genfromtxt('data_q0_train.csv', delimiter=',')\n",
    "x_train = data_train[:,0].reshape(-1, 1)\n",
    "y_train = data_train[:,1].reshape(-1, 1)\n",
    "\n",
    "data_test = np.genfromtxt('data_q0_test.csv', delimiter=',')\n",
    "x_test = data_test[:,0].reshape(-1, 1)\n",
    "y_test = data_test[:,1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a model that can predict y for unknown inputs x.\n",
    "\n",
    "(a) (5pts) Fit a linear model to the training data, and report mean squared error on the test data. Plot the data, fitted model, and predictions, clearly denoting the training, testing, and predicted points.\n",
    "\n",
    "(b) (5pts) Fit polynomial models of degrees 1, 2, 3, and 5 to the training data, and report mean squared error for both models. Plot the data, the fitted models, and the predicted outputs.\n",
    "\n",
    "(c) (5pts) Which model performed the best? Explain using the bias-variance tradeoff.\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1. Data Exploration (23pts)\n",
    "Use the following code to import the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#read the data\n",
    "data = pd.read_csv('data_q1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "  * TV: advertising dollars spent on TV for a single product (in thousands of dollars)\n",
    "  * Radio: advertising dollars spent on Radio\n",
    "  * Newspaper: advertising dollars spent on Newspaper\n",
    "  * Sales (dependent variable): sales of a single product in a given market (in thousands of widgets)\n",
    "\n",
    "We are interested in predicting sales based on the first three \"feature\" variables (TV, Radio and Newspaper).\n",
    "\n",
    "(a) (2pt) Print the shape (number of rows and columns) of the data matrix and show the first 5 rows.\n",
    "\n",
    "(b) (4pts) Generate box-plots for each of the four columns and identify the cutoff values for outliers. \n",
    "\n",
    "(c) (4pts) Visualize the relationship between the features and the response variable (Sales) using scatterplots. Comment on the fits.\n",
    "\n",
    "(d) (4pts) Fit a simple linear regression of 'Sales' on 'TV'. What is the regression coefficient for 'TV'? What is its interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the data randomly into a training and test set (keep one third of the data for test).\n",
    "\n",
    "(e) (4pts) Fit an MLR on all the feature variables using the training data and evaluate the trained model on the test data using root mean squared error.\n",
    "\n",
    "(f) (3pts) Report the MSE obtained on train data. How much does this increase when you score your model on test data?\n",
    "\n",
    "(g) (2pts) Report the coefficients obtained by your model.\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. Regression with Feature Selection and Outlier Detection (20pts)\n",
    "\n",
    "In this problem, we will build a predictive model to estimate the market price using the Dow-Jones index (https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average). To simplify the problem, we will use previous weeks' price and volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>percent_change_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>previous_weeks_volume</th>\n",
       "      <th>percent_change_volume_over_last_wk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$16.71</td>\n",
       "      <td>$16.71</td>\n",
       "      <td>$15.64</td>\n",
       "      <td>$15.97</td>\n",
       "      <td>-4.428490</td>\n",
       "      <td>242963398</td>\n",
       "      <td>239655616.0</td>\n",
       "      <td>1.380223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$16.19</td>\n",
       "      <td>$16.38</td>\n",
       "      <td>$15.60</td>\n",
       "      <td>$15.79</td>\n",
       "      <td>-2.470660</td>\n",
       "      <td>138428495</td>\n",
       "      <td>242963398.0</td>\n",
       "      <td>-43.024959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$15.87</td>\n",
       "      <td>$16.63</td>\n",
       "      <td>$15.82</td>\n",
       "      <td>$16.13</td>\n",
       "      <td>1.638310</td>\n",
       "      <td>151379173</td>\n",
       "      <td>138428495.0</td>\n",
       "      <td>9.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$16.18</td>\n",
       "      <td>$17.39</td>\n",
       "      <td>$16.18</td>\n",
       "      <td>$17.14</td>\n",
       "      <td>5.933250</td>\n",
       "      <td>154387761</td>\n",
       "      <td>151379173.0</td>\n",
       "      <td>1.987452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$17.33</td>\n",
       "      <td>$17.48</td>\n",
       "      <td>$16.97</td>\n",
       "      <td>$17.37</td>\n",
       "      <td>0.230814</td>\n",
       "      <td>114691279</td>\n",
       "      <td>154387761.0</td>\n",
       "      <td>-25.712195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     open    high     low   close  percent_change_price     volume  \\\n",
       "1  $16.71  $16.71  $15.64  $15.97             -4.428490  242963398   \n",
       "2  $16.19  $16.38  $15.60  $15.79             -2.470660  138428495   \n",
       "3  $15.87  $16.63  $15.82  $16.13              1.638310  151379173   \n",
       "4  $16.18  $17.39  $16.18  $17.14              5.933250  154387761   \n",
       "5  $17.33  $17.48  $16.97  $17.37              0.230814  114691279   \n",
       "\n",
       "   previous_weeks_volume  percent_change_volume_over_last_wk  \n",
       "1            239655616.0                            1.380223  \n",
       "2            242963398.0                          -43.024959  \n",
       "3            138428495.0                            9.355500  \n",
       "4            151379173.0                            1.987452  \n",
       "5            154387761.0                          -25.712195  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('dow_jones_index.data')\n",
    "df = df.dropna()\n",
    "X_df = df[[\n",
    "        u'open', u'high', u'low', u'close',\n",
    "        u'percent_change_price',\n",
    "        u'volume', \n",
    "        u'previous_weeks_volume',\n",
    "        u'percent_change_volume_over_last_wk']]\n",
    "\n",
    "feature_name = list(X_df.columns.values)\n",
    "\n",
    "# check the first 5 rows of data loaded\n",
    "pd.DataFrame.head(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward/ backward feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (3pts) Calculate the MSE of the training set using linear regression.\n",
    "\n",
    "(b) (3pts) Find the three most significant factors using (1) forward feature selection and (2) backward feature selection. Please install and use MLXTEND package for above feature selections (https://anaconda.org/conda-forge/mlxtend). You may find the following link helpful: https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#example-5-sequential-feature-selection-for-regression. Please set the cross-validation parameter to 3 (cv=3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) (2pt) Are the three most significant features found using forward and backward feature selection the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss function and outlier\n",
    "\n",
    "In the second part, we will fit a linear model to the data, using a Huber loss function rather than the L2 norm usually used in OLS. sklearn has a nice API you can use: [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor). For this problem, we will consider only one feature: \"percent\\_change\\_price\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('dow_jones_index.data')\n",
    "stock = df[[\"percent_change_price\", \"next_weeks_open\"]]\n",
    "stock = stock.dropna()\n",
    "stock = stock.values[1:]\n",
    "stock[:, 1] = [i.split(\"$\")[1] for i in stock[:, 1]]\n",
    "stock = stock.astype('float')\n",
    "stock[:, 0] = (stock[:, 0] - np.mean(stock[:, 0], axis=0))/np.std(stock[:, 0], axis=0)\n",
    "\n",
    "X = stock[:,:1]\n",
    "y = stock[:, 1]\n",
    "\n",
    "X_train = X[:400,]\n",
    "y_train = y[:400]\n",
    "\n",
    "X_test = X[400:,]\n",
    "y_test = y[400:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) (2pts) Calculate the MSE using linear regression (linear_model.LinearRegression) using this single feature. Please do not use any regularization coefficient (set $\\lambda=0$).\n",
    "\n",
    "(e) (2pts) Calculate the MSE using Huber regression (linear_model.HuberRegressor) and compare with the result obtained in (d).\n",
    "\n",
    "Now we purposefully insert an outlier into the training set. Please note that this is simply for academic purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_outliers = np.copy(y_train)\n",
    "y_train_outliers[0] = 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) (3pts) Explain the difference in predictive performance of the two models in these two scenarios: with and without an outlier.\n",
    "\n",
    "(g) (5pts) As you have previously done, please create two models, logistric regression and huber regression, but using the new target vector during training. Please print the test and train MSE (5 pts).\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3. Sampling (6pts)\n",
    "\n",
    "A recent survey estimated that $30\\%$ of all Europeans aged 20 to 22 have driven under the influence of drugs or alcohol, based on a simple \"Yes or No\" question. A similar survey is being planned for Americans. The survey designers want the  $90\\%$ confidence interval to have a margin of error of at most $\\pm0.09$.\n",
    "\n",
    "(a) (2pts) Find the necessary sample size needed to conduct this survey assuming that the expected percentage of \"yes\" answers will be very close to that obtained from the European survey?\n",
    "\n",
    "(b) (2pts) Suppose the tolerance level was kept the same but the confidence level needs to increase to $95\\%$. What is the required sample size for this new specification?\n",
    "\n",
    "(c) (2pts) If one does not know where the true \"$p$\" may lie, one can conservatively conduct a survey assuming the worst case (in terms of required minimum sample size)  scenario of  $p = 0.5$. Redo part (b) for this \"worst case\" scenario.\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4. Principal Component Analysis (11pts)\n",
    "\n",
    "Use the following code to read in data of US Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agricultural machinery, equipment</th>\n",
       "      <th>Alcoholic beverages, excluding wine</th>\n",
       "      <th>Apparel, household goods - cotton</th>\n",
       "      <th>Apparel, household goods - wool</th>\n",
       "      <th>Apparel, textiles, nonwool or cotton</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>3105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10739.0</td>\n",
       "      <td>7314.0</td>\n",
       "      <td>11942.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34741.0</td>\n",
       "      <td>2752171.0</td>\n",
       "      <td>50838.0</td>\n",
       "      <td>1298224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andorra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24505.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Agricultural machinery, equipment  \\\n",
       "Country                                          \n",
       "Afghanistan                             3105.0   \n",
       "Albania                                    0.0   \n",
       "Algeria                                    0.0   \n",
       "Andorra                                    0.0   \n",
       "Angola                                     0.0   \n",
       "\n",
       "             Alcoholic beverages, excluding wine  \\\n",
       "Country                                            \n",
       "Afghanistan                                  0.0   \n",
       "Albania                                  34741.0   \n",
       "Algeria                                      0.0   \n",
       "Andorra                                      0.0   \n",
       "Angola                                   24505.0   \n",
       "\n",
       "             Apparel, household goods - cotton  \\\n",
       "Country                                          \n",
       "Afghanistan                            10739.0   \n",
       "Albania                              2752171.0   \n",
       "Algeria                                    0.0   \n",
       "Andorra                                  351.0   \n",
       "Angola                                     0.0   \n",
       "\n",
       "             Apparel, household goods - wool  \\\n",
       "Country                                        \n",
       "Afghanistan                           7314.0   \n",
       "Albania                              50838.0   \n",
       "Algeria                                  0.0   \n",
       "Andorra                                  0.0   \n",
       "Angola                                   0.0   \n",
       "\n",
       "             Apparel, textiles, nonwool or cotton  \n",
       "Country                                            \n",
       "Afghanistan                               11942.0  \n",
       "Albania                                 1298224.0  \n",
       "Algeria                                       0.0  \n",
       "Andorra                                       0.0  \n",
       "Angola                                        0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_q4.csv', index_col=0)\n",
    "df.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (3pts) Find the top two principal components from this dataset, and make a scatter plot with the first component as the x-axis and the second as the y-axis. You may find the sklearn PCA package to be useful.\n",
    "\n",
    "(b) (2pts) Find the names of the six countries with the highest first component (these should be clear outliers).\n",
    "\n",
    "(c) (3pts) Create i) a scree plot depicting the proportion of variance and ii) a cumulative proportion of variance explained by the principal components of the data (X matrix).  Refer to Figure 10.4 of JW for an example.  If you are using sklearn's PCA implementation, you may use the output attribute *explained variance ratio*.\n",
    "\n",
    "(d) (3pts) How many principal components are required to explain cumulative variance of 30%, 60%, and 90%, respectively?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5. PCA (conceptual) (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (5pts) Give two reasons why we might want to use PCA.\n",
    "\n",
    "(b) (5pts) If we approach PCA using eigenvalue decomposition on the covariance matrix, explain what the eigenvectors and eigenvalues represent.\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
